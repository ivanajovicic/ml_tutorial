{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy Version: 1.16.4\n",
      "pandas Version: 0.24.2\n",
      "Scikit-Learn Version: 0.21.2\n",
      "XGBoost Version: 1.0.0rapidsdev0-SNAPSHOT\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np; print('numpy Version:', np.__version__)\n",
    "import pandas as pd; print('pandas Version:', pd.__version__)\n",
    "import sklearn; print('Scikit-Learn Version:', sklearn.__version__)\n",
    "import xgboost as xgb; print('XGBoost Version:', xgb.__version__)\n",
    "\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook we already load our data and split it in training .\n",
    "We intentionally named the objects we created in a way you can easy distingwish whether you are using pandas(`pd_`) vs cudf gpu (`cdf_`) data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "%run ../files/load_data.py\n",
    "time_needeed = start - time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is to convert this to a DMatrix object that XGBoost can work with. We can instantiate an object of the `xgboost.DMatrix` by passing in the feature matrix as the first argument followed by the label vector using the `label= keyword argument`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(pd_X_train, label = np.squeeze(pd_y_train))\n",
    "dvalidation = xgb.DMatrix(pd_X_test, label = np.squeeze(pd_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data dimension :  800 100\n",
      "Validation data dimension: 200 100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# check dimensions\n",
    "print('Training data dimension : ', dtrain.num_row(), dtrain.num_col())\n",
    "print('Validation data dimension:', dvalidation.num_row(), dvalidation.num_col())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'silent': 1, 'tree_method': 'gpu_hist', 'n_gpus': -1, 'eval_metric': 'auc', 'objective': 'binary:logistic'}\n"
     ]
    }
   ],
   "source": [
    "# instantiate params\n",
    "params = {}\n",
    "\n",
    "# general params\n",
    "general_params = {'silent': 1}\n",
    "params.update(general_params)\n",
    "\n",
    "# booster params\n",
    "n_gpus = -1  # this means \"use all the GPUs available. Change it to 1 to use single GPU or 0 to use the CPU\n",
    "booster_params = {}\n",
    "\n",
    "if n_gpus != 0:\n",
    "    booster_params['tree_method'] = 'gpu_hist'\n",
    "    booster_params['n_gpus'] = n_gpus   \n",
    "params.update(booster_params)\n",
    "\n",
    "# learning task params\n",
    "learning_task_params = {}\n",
    "learning_task_params['eval_metric'] = 'auc'\n",
    "learning_task_params['objective'] = 'binary:logistic'\n",
    "\n",
    "params.update(learning_task_params)\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train XGBoost model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Monitor gpu utilization** by typing `watch 0.1 nvidia-smi` in your console "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to pass a `num_boost_round` which corresponds to the maximum number of boosting rounds that we allow. We want to set it to a large value hoping to find the optimal number of rounds before reaching it, if we haven't improved performance on our test dataset in `early_stopping_round` rounds.\n",
    "\n",
    "You can try setting a larger number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 µs, sys: 2 µs, total: 3 µs\n",
      "Wall time: 6.91 µs\n",
      "[0]\tvalidation-auc:0.766216\ttrain-auc:0.956076\n",
      "Multiple eval metrics have been passed: 'train-auc' will be used for early stopping.\n",
      "\n",
      "Will train until train-auc hasn't improved in 10 rounds.\n",
      "[5]\tvalidation-auc:0.896942\ttrain-auc:0.999537\n",
      "[10]\tvalidation-auc:0.918997\ttrain-auc:1\n",
      "[15]\tvalidation-auc:0.924511\ttrain-auc:1\n",
      "Stopping. Best iteration:\n",
      "[9]\tvalidation-auc:0.910576\ttrain-auc:1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%time \n",
    "startTime = time.time()\n",
    "\n",
    "evallist = [(dvalidation, 'validation'), (dtrain, 'train')]\n",
    "\n",
    "gpu_train = xgb.train(\n",
    "    params = params, \n",
    "    dtrain = dtrain,\n",
    "    evals = evallist,\n",
    "    num_boost_round = 100, #maximum number of boosting rounds we allow\n",
    "    verbose_eval = 5,\n",
    "    early_stopping_rounds = 10 # The number of rounds without improvements after which we should stop,\n",
    ") \n",
    "\n",
    "gpuTrainTime = time.time() - startTime "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: \n",
    "\n",
    "Change your parameter setings to train this model **without usign GPU** and check the speedup time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'silent': 1, 'eval_metric': 'auc', 'objective': 'binary:logistic'}\n"
     ]
    }
   ],
   "source": [
    "## SOLUTION\n",
    "\n",
    "# instantiate params\n",
    "params = {}\n",
    "\n",
    "# general params\n",
    "general_params = {'silent': 1}\n",
    "params.update(general_params)\n",
    "\n",
    "# booster params\n",
    "n_gpus = 0  # this means \"use all the GPUs available. Change it to 1 to use single GPU or 0 to use the CPU\n",
    "booster_params = {}\n",
    "\n",
    "if n_gpus != 0:\n",
    "    booster_params['tree_method'] = 'gpu_hist'\n",
    "    booster_params['n_gpus'] = n_gpus   \n",
    "params.update(booster_params)\n",
    "\n",
    "# learning task params\n",
    "learning_task_params = {}\n",
    "learning_task_params['eval_metric'] = 'auc'\n",
    "learning_task_params['objective'] = 'binary:logistic'\n",
    "\n",
    "params.update(learning_task_params)\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 6.68 µs\n",
      "[0]\tvalidation-auc:0.795739\ttrain-auc:0.953388\n",
      "Multiple eval metrics have been passed: 'train-auc' will be used for early stopping.\n",
      "\n",
      "Will train until train-auc hasn't improved in 10 rounds.\n",
      "[5]\tvalidation-auc:0.922406\ttrain-auc:0.999318\n",
      "[10]\tvalidation-auc:0.938145\ttrain-auc:1\n",
      "[15]\tvalidation-auc:0.936942\ttrain-auc:1\n",
      "[20]\tvalidation-auc:0.94015\ttrain-auc:1\n",
      "Stopping. Best iteration:\n",
      "[10]\tvalidation-auc:0.938145\ttrain-auc:1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%time \n",
    "startTime = time.time()\n",
    "evallist = [(dvalidation, 'validation'), (dtrain, 'train')]\n",
    "cpu_train = xgb.train(\n",
    "    params = params, \n",
    "    dtrain = dtrain,\n",
    "    evals = evallist,\n",
    "    num_boost_round = 100,\n",
    "    verbose_eval = 5,\n",
    "    early_stopping_rounds = 10 )\n",
    "\n",
    "\n",
    "cpuTrainTime = time.time() - startTime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02197543131091485"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpuTrainTime/gpuTrainTime "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.datacamp.com/community/tutorials/xgboost-in-python#what\n",
    "\n",
    "https://blog.cambridgespark.com/hyperparameter-tuning-in-xgboost-4ff9100a3b2f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
