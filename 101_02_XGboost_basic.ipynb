{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy Version: 1.16.4\n",
      "pandas Version: 0.24.2\n",
      "Scikit-Learn Version: 0.21.2\n",
      "XGBoost Version: 1.0.0rapidsdev0-SNAPSHOT\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np; print('numpy Version:', np.__version__)\n",
    "import pandas as pd; print('pandas Version:', pd.__version__)\n",
    "import sklearn; print('Scikit-Learn Version:', sklearn.__version__)\n",
    "import xgboost as xgb; print('XGBoost Version:', xgb.__version__)\n",
    "\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook we already load our data and split it in training .\n",
    "We intentionally named the objects we created in a way you can easy distingwish whether you are using pandas(`pd_`) vs cudf gpu (`cdf_`) data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 51s, sys: 14.7 s, total: 4min 6s\n",
      "Wall time: 4min 8s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "%run files/load_data.py\n",
    "time_needeed = start - time.time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is to convert this to a DMatrix object that XGBoost can work with. We can instantiate an object of the `xgboost.DMatrix` by passing in the feature matrix as the first argument followed by the label vector using the `label= keyword argument`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(pd_X_train, label = np.squeeze(pd_y_train))\n",
    "dvalidation = xgb.DMatrix(pd_X_test, label = np.squeeze(pd_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data dimension :  8000000 100\n",
      "Validation data dimension: 2000000 100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# check dimensions\n",
    "print('Training data dimension : ', dtrain.num_row(), dtrain.num_col())\n",
    "print('Validation data dimension:', dvalidation.num_row(), dvalidation.num_col())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'silent': 1, 'tree_method': 'gpu_hist', 'n_gpus': -1, 'eval_metric': 'auc', 'objective': 'binary:logistic'}\n"
     ]
    }
   ],
   "source": [
    "# instantiate params\n",
    "params = {}\n",
    "\n",
    "# general params\n",
    "general_params = {'silent': 1}\n",
    "params.update(general_params)\n",
    "\n",
    "# booster params\n",
    "n_gpus = -1  # this means \"use all the GPUs available. Change it to 1 to use single GPU or 0 to use the CPU\n",
    "booster_params = {}\n",
    "\n",
    "if n_gpus != 0:\n",
    "    booster_params['tree_method'] = 'gpu_hist'\n",
    "    booster_params['n_gpus'] = n_gpus   \n",
    "params.update(booster_params)\n",
    "\n",
    "# learning task params\n",
    "learning_task_params = {}\n",
    "learning_task_params['eval_metric'] = 'auc'\n",
    "learning_task_params['objective'] = 'binary:logistic'\n",
    "\n",
    "params.update(learning_task_params)\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train XGBoost model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Monitor gpu utilization** by typing `watch 0.1 nvidia-smi` in your console "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to pass a `num_boost_round` which corresponds to the maximum number of boosting rounds that we allow. We want to set it to a large value hoping to find the optimal number of rounds before reaching it, if we haven't improved performance on our test dataset in `early_stopping_round` rounds.\n",
    "\n",
    "You can try setting a larger number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9 µs, sys: 0 ns, total: 9 µs\n",
      "Wall time: 17.4 µs\n",
      "[0]\tvalidation-auc:0.885117\ttrain-auc:0.885372\n",
      "Multiple eval metrics have been passed: 'train-auc' will be used for early stopping.\n",
      "\n",
      "Will train until train-auc hasn't improved in 10 rounds.\n",
      "[5]\tvalidation-auc:0.956869\ttrain-auc:0.956969\n",
      "[10]\tvalidation-auc:0.974715\ttrain-auc:0.974869\n",
      "[15]\tvalidation-auc:0.981039\ttrain-auc:0.981213\n",
      "[20]\tvalidation-auc:0.984458\ttrain-auc:0.984653\n",
      "[25]\tvalidation-auc:0.986609\ttrain-auc:0.986795\n",
      "[30]\tvalidation-auc:0.989126\ttrain-auc:0.989285\n",
      "[35]\tvalidation-auc:0.990134\ttrain-auc:0.990286\n",
      "[40]\tvalidation-auc:0.990911\ttrain-auc:0.991067\n",
      "[45]\tvalidation-auc:0.9918\ttrain-auc:0.991945\n",
      "[50]\tvalidation-auc:0.992184\ttrain-auc:0.992335\n",
      "[55]\tvalidation-auc:0.992568\ttrain-auc:0.992718\n",
      "[60]\tvalidation-auc:0.99295\ttrain-auc:0.993105\n",
      "[65]\tvalidation-auc:0.993174\ttrain-auc:0.993328\n",
      "[70]\tvalidation-auc:0.993267\ttrain-auc:0.993428\n",
      "[75]\tvalidation-auc:0.993331\ttrain-auc:0.993504\n",
      "[80]\tvalidation-auc:0.993469\ttrain-auc:0.993649\n",
      "[85]\tvalidation-auc:0.993659\ttrain-auc:0.993851\n",
      "[90]\tvalidation-auc:0.993823\ttrain-auc:0.994026\n",
      "[95]\tvalidation-auc:0.993961\ttrain-auc:0.994164\n",
      "[99]\tvalidation-auc:0.994069\ttrain-auc:0.994279\n"
     ]
    }
   ],
   "source": [
    "%time \n",
    "startTime = time.time()\n",
    "\n",
    "evallist = [(dvalidation, 'validation'), (dtrain, 'train')]\n",
    "\n",
    "gpu_train = xgb.train(\n",
    "    params = params, \n",
    "    dtrain = dtrain,\n",
    "    evals = evallist,\n",
    "    num_boost_round = 100, #maximum number of boosting rounds we allow\n",
    "    verbose_eval = 5,\n",
    "    early_stopping_rounds = 10 # The number of rounds without improvements after which we should stop,\n",
    ") \n",
    "\n",
    "gpuTrainingTime = time.time() - startTime "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: \n",
    "\n",
    "Change your parameter setings to train this model **without usign GPU** and check the speedup time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'silent': 1, 'eval_metric': 'auc', 'objective': 'binary:logistic'}\n"
     ]
    }
   ],
   "source": [
    "## SOLUTION\n",
    "\n",
    "# instantiate params\n",
    "params = {}\n",
    "\n",
    "# general params\n",
    "general_params = {'silent': 1}\n",
    "params.update(general_params)\n",
    "\n",
    "# booster params\n",
    "n_gpus = 0  # this means \"use all the GPUs available. Change it to 1 to use single GPU or 0 to use the CPU\n",
    "booster_params = {}\n",
    "\n",
    "if n_gpus != 0:\n",
    "    booster_params['tree_method'] = 'gpu_hist'\n",
    "    booster_params['n_gpus'] = n_gpus   \n",
    "params.update(booster_params)\n",
    "\n",
    "# learning task params\n",
    "learning_task_params = {}\n",
    "learning_task_params['eval_metric'] = 'auc'\n",
    "learning_task_params['objective'] = 'binary:logistic'\n",
    "\n",
    "params.update(learning_task_params)\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time \n",
    "# startTime = time.time()\n",
    "# evallist = [(dvalidation, 'validation'), (dtrain, 'train')]\n",
    "# cpu_train = xgb.train(\n",
    "#     params = params, \n",
    "#     dtrain = dtrain,\n",
    "#     evals = evallist,\n",
    "#     num_boost_round = 10,\n",
    "#     verbose_eval = 5,\n",
    "#     early_stopping_rounds = 10 )\n",
    "\n",
    "\n",
    "# cpuTrainTime = time.time() - startTime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpuTrainTime/gpuTrainTime "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.datacamp.com/community/tutorials/xgboost-in-python#what\n",
    "\n",
    "https://blog.cambridgespark.com/hyperparameter-tuning-in-xgboost-4ff9100a3b2f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
