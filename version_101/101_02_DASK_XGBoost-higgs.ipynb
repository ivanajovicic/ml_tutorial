{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DASK XGBoost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction to Dask and Dask cuDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Dask](https://dask.org/) is a Python library for parallel computing. In Dask programming, we create computational graphs that define code we **would like** to execute, and then, give these computational graphs to a Dask scheduler which evaluates them lazily, and efficiently, in parallel. In addition to using multiple CPU cores or threads to execute computational graphs in parallel, Dask schedulers can also be configured to execute computational graphs on multiple CPUs, or, as we will do in this workshop, multiple GPUs. On account of its ability to utilize multiple compute resources, Dask programming facilitates operating on datasets that are larger than the memory of a single compute resource.\n",
    "\n",
    "[Dask cuDF](https://github.com/rapidsai/dask-cudf) can be used to distribute dataframe operations on larger-than-memory datasets to multiple GPUs. In this notebook you'll receive and introduction to some key Dask concepts, learn how to setup a Dask cluster for utilizing multiple GPUs, and how to perform simple dataframe operations on distributed Dask dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up a Dask Scheduler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by starting a Dask scheduler which will take care to distribute our work across the 4 available GPUs. In order to do this we need to start a `LocalCUDACluster` instance, using our host machine's IP, and then instantiate a client that can communicate with the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192.168.99.2\n"
     ]
    }
   ],
   "source": [
    "import subprocess # ...which we will use to obtain our local IP using the cmd...\n",
    "cmd = \"hostname --all-ip-addresses\"\n",
    "\n",
    "process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)\n",
    "output, error = process.communicate()\n",
    "IPADDR = str(output.decode()).split()[0]\n",
    "print(IPADDR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starting a `LocalCUDACluster`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dask_cuda` provides utilities for Dask and CUDA (as in **cu**DF) interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask import delayed\n",
    "from dask_cuda import LocalCUDACluster\n",
    "cluster = LocalCUDACluster(ip=IPADDR)\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating a Client Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dask.distributed` library gives us distributed functionality, including the ability to connect to the CUDA Cluster we just created. The `progress` import will give us a handy progress bar we can utilize below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://192.168.99.2:34645</li>\n",
       "  <li><b>Dashboard: </b><a href='http://192.168.99.2:8787/status' target='_blank'>http://192.168.99.2:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>270.39 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://192.168.99.2:34645' processes=4 cores=4>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client, progress\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "918e45a31a0d4b9b8ec649d0fc8ee61f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>LocalCUDACluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\nâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dask Dashboard \n",
    "As you can see, the `client` instance gives us information about our CUDA cluster (utilizing 4 GPUs), as well as information about our client connection. Dask ships with an incredibly helpful dashboard, which you can see runs on port `8787`. Open a new browser tab now at `<YOUR_IP_ADDRESS>:8787`, for example `ec2-12-345-67-890.us-east-2.compute.amazonaws.com:8787`, which should open the Dask dashboard, currently idle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy Version: 1.16.2\n",
      "pandas Version: 0.24.2\n",
      "Scikit-Learn Version: 0.20.4\n",
      "Dask XGBoost Version: 0.1.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'rapids_lib_v8' from '/rapids/notebooks/ml_tutorial/version_101/rapids_lib_v8.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install dask_xgboost\n",
    "import numpy as np; print('numpy Version:', np.__version__)\n",
    "import pandas as pd; print('pandas Version:', pd.__version__)\n",
    "\n",
    "import sklearn; print('Scikit-Learn Version:', sklearn.__version__)\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import dask_xgboost; print('Dask XGBoost Version:', dask_xgboost.__version__)\n",
    "import dask_cudf\n",
    "\n",
    "import time \n",
    "\n",
    "\n",
    "import rapids_lib_v8 as rl\n",
    "''' NOTE: anytime changes are made to rapids_lib.py you can either:\n",
    "      1. refresh/reload via the code below, OR\n",
    "      2. restart the kernel '''\n",
    "import importlib; importlib.reload(rl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r expLog\n",
    "%store -r trainData_pDF\n",
    "%store -r trainLabels_pDF \n",
    "%store -r testData_pDF\n",
    "%store -r testLabels_pDF\n",
    "\n",
    "%store -r expLog\n",
    "%store -r trainData_cDF\n",
    "%store -r trainLabels_cDF \n",
    "%store -r testData_cDF\n",
    "%store -r testLabels_cDF\n",
    "\n",
    "nCores = !nproc --all\n",
    "nCores = int(nCores[0]) # we want to extract number of cores the CPU has \n",
    "\n",
    "paramsCPU = {\n",
    "    'max_depth': 10,\n",
    "    'learning_rate': .1,\n",
    "    'num_boost_rounds': 100,\n",
    "    'lambda': 1,\n",
    "    'objective': 'binary:hinge',\n",
    "    'tree_method': 'hist',\n",
    "    'n_jobs': nCores,\n",
    "    'random_state': 0\n",
    "}\n",
    "\n",
    "paramsGPU = {\n",
    "    'max_depth': 10,\n",
    "    'learning_rate': .1,\n",
    "    'num_boost_rounds': 100,\n",
    "    'lambda': 1,\n",
    "    'objective': 'binary:hinge',\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'n_gpus': 1,    \n",
    "    'random_state': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_partitions = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    " #create Dask cuDF dataframes\n",
    "trainData_dask_cDF = dask_cudf.from_cudf(trainData_cDF, npartitions = num_partitions )\n",
    "testData_dask_cDF = dask_cudf.from_cudf(testData_cDF, npartitions = num_partitions)\n",
    "trainLabels_dask_cDF = dask_cudf.from_cudf(trainLabels_cDF, npartitions = num_partitions)\n",
    "testLabels_dask_cDF = dask_cudf.from_cudf(testLabels_cDF, npartitions = num_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData_dask_cDF= trainData_dask_cDF.persist()\n",
    "testData_dask_cDF=testData_dask_cDF.persist()\n",
    "trainLabels_dask_cDF = trainLabels_dask_cDF.persist()\n",
    "testLabels_dask_cDF =testLabels_dask_cDF.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dask_cudf.core.DataFrame"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(testLabels_dask_cDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Dask DataFrames\n",
    "trainData_dask_pDF = dask.dataframe.from_pandas(trainData_pDF, npartitions = num_partitions)\n",
    "testData_dask_pDF = dask.dataframe.from_pandas(testData_pDF, npartitions = num_partitions)\n",
    "trainLabels_dask_pDF = dask.dataframe.from_pandas(trainLabels_pDF, npartitions=num_partitions)\n",
    "testLabels_dask_pDF=dask.dataframe.from_pandas(testLabels_pDF,npartitions = num_partitions)\n",
    "\n",
    "# #create Dask cuDF dataframes\n",
    "# trainData_dask_cDF = dask_cudf.from_dask_dataframe(trainData_dask_pDF)\n",
    "# testData_dask_cDF = dask_cudf.from_dask_dataframe(testData_dask_pDF)\n",
    "# trainLabels_dask_cDF = dask_cudf.from_dask_dataframe(trainLabels_dask_pDF)\n",
    "# testLabels_dask_cDF = dask_cudf.from_dask_dataframe(testLabels_dask_pDF)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Dask data prep:\n",
    "\n",
    "#scatter data:\n",
    "# scattaredData_future=client.scatter([trainData_pDF, trainLabels_pDF, testData_pDF, testLabels_pDF],broadcast=True )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(scattaredData_future[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainData_pDF_future = scattaredData_future[0]; \n",
    "# trainLabels_pDF_future = scattaredData_future[1]\n",
    "# testData_pDF_future = scattaredData_future[2]; \n",
    "# testLabels_pDF_future = scattaredData_future[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(dask_xgboost.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-2ae71b5f22c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                      \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrainData_dask_pDF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                      \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainLabels_dask_pDF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                                      num_boost_round = paramsCPU['num_boost_rounds'])\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CPU training time:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstartTime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"seconds.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.6/site-packages/dask_xgboost/core.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(client, params, data, labels, dmatrix_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \"\"\"\n\u001b[1;32m    234\u001b[0m     return client.sync(_train, client, params, data,\n\u001b[0;32m--> 235\u001b[0;31m                        labels, dmatrix_kwargs, **kwargs)\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.6/site-packages/distributed/client.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(self, func, asynchronous, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m             return sync(\n\u001b[0;32m--> 753\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m             )\n\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.6/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "''' -------------------------------------------------------------------------\n",
    ">  CPU Train and Test\n",
    "------------------------------------------------------------------------- '''\n",
    "startTime = time.time()\n",
    "Dask_xgBoostModelCPU = dask_xgboost.train( client=client, \n",
    "                                     params = paramsCPU,\n",
    "                                     data= trainData_dask_pDF, \n",
    "                                     labels = trainLabels_dask_pDF,\n",
    "                                     num_boost_round = paramsCPU['num_boost_rounds'])\n",
    "\n",
    "print(\"CPU training time:\" + str(time.time()-startTime),\"seconds.\")\n",
    "\n",
    "#Generate Predictions\n",
    "predictionsCPU = dask_xgboost.predict(client, Dask_xgBoostModelCPU, testData_dask_pDF)\n",
    "predictionsCPU_con = dask.dataframe.multi.concat([predictionsCPU], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU training time:38.457282066345215 seconds.\n"
     ]
    }
   ],
   "source": [
    "''' -------------------------------------------------------------------------\n",
    ">  GPU Train and Test\n",
    "------------------------------------------------------------------------- '''\n",
    "startTime = time.time()\n",
    "Dask_xgBoostModelGPU = dask_xgboost.train( client=client, \n",
    "                                     params = paramsGPU,\n",
    "                                     data= trainData_dask_pDF, \n",
    "                                     labels = trainLabels_dask_pDF,\n",
    "                                     num_boost_round = paramsGPU['num_boost_rounds'])\n",
    "\n",
    "print(\"GPU training time:\" + str(time.time()-startTime),\"seconds.\")\n",
    "\n",
    "# #Generate Predictions\n",
    "predictionsGPU = dask_xgboost.predict(client, Dask_xgBoostModelGPU, testData_dask_pDF)\n",
    "predictionsGPU_con = dask.dataframe.multi.concat([predictionsGPU], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(predictionsGPU_con.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_accuracy = accuracy_score(testLabels_dask_pDF.astype(int),predictionsGPU_con.compute());print(gpu_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_accuracy = accuracy_score(testLabels_dask_pDF.astype(int),predictionsCPU_con.compute());print(cpu_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
