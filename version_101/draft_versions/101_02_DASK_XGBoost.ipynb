{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DASK XGBoost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction to Dask and Dask cuDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Dask](https://dask.org/) is a Python library for parallel computing. In Dask programming, we create computational graphs that define code we **would like** to execute, and then, give these computational graphs to a Dask scheduler which evaluates them lazily, and efficiently, in parallel. In addition to using multiple CPU cores or threads to execute computational graphs in parallel, Dask schedulers can also be configured to execute computational graphs on multiple CPUs, or, as we will do in this workshop, multiple GPUs. On account of its ability to utilize multiple compute resources, Dask programming facilitates operating on datasets that are larger than the memory of a single compute resource.\n",
    "\n",
    "[Dask cuDF](https://github.com/rapidsai/dask-cudf) can be used to distribute dataframe operations on larger-than-memory datasets to multiple GPUs. In this notebook you'll receive and introduction to some key Dask concepts, learn how to setup a Dask cluster for utilizing multiple GPUs, and how to perform simple dataframe operations on distributed Dask dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up a Dask Scheduler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by starting a Dask scheduler which will take care to distribute our work across the 4 available GPUs. In order to do this we need to start a `LocalCUDACluster` instance, using our host machine's IP, and then instantiate a client that can communicate with the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192.168.99.2\n"
     ]
    }
   ],
   "source": [
    "import subprocess # ...which we will use to obtain our local IP using the cmd...\n",
    "cmd = \"hostname --all-ip-addresses\"\n",
    "\n",
    "process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)\n",
    "output, error = process.communicate()\n",
    "IPADDR = str(output.decode()).split()[0]\n",
    "print(IPADDR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starting a `LocalCUDACluster`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dask_cuda` provides utilities for Dask and CUDA (as in **cu**DF) interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.6/site-packages/distributed/dashboard/core.py:72: UserWarning: \n",
      "Port 8787 is already in use. \n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the diagnostics dashboard on a random port instead.\n",
      "  warnings.warn(\"\\n\" + msg)\n"
     ]
    }
   ],
   "source": [
    "import dask\n",
    "from dask import delayed\n",
    "from dask_cuda import LocalCUDACluster\n",
    "cluster = LocalCUDACluster(ip=IPADDR)\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating a Client Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dask.distributed` library gives us distributed functionality, including the ability to connect to the CUDA Cluster we just created. The `progress` import will give us a handy progress bar we can utilize below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://192.168.99.2:35613</li>\n",
       "  <li><b>Dashboard: </b><a href='http://192.168.99.2:33417/status' target='_blank'>http://192.168.99.2:33417/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>270.39 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://192.168.99.2:35613' processes=4 cores=4>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client, progress\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dask Dashboard \n",
    "As you can see, the `client` instance gives us information about our CUDA cluster (utilizing 4 GPUs), as well as information about our client connection. Dask ships with an incredibly helpful dashboard, which you can see runs on port `8787`. Open a new browser tab now at `<YOUR_IP_ADDRESS>:8787`, for example `ec2-12-345-67-890.us-east-2.compute.amazonaws.com:8787`, which should open the Dask dashboard, currently idle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy Version: 1.16.2\n",
      "pandas Version: 0.24.2\n",
      "Scikit-Learn Version: 0.20.4\n",
      "Dask XGBoost Version: 0.1.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'rapids_lib_v8' from '/rapids/notebooks/ml_tutorial/version_101/rapids_lib_v8.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install dask_xgboost\n",
    "import numpy as np; print('numpy Version:', np.__version__)\n",
    "import pandas as pd; print('pandas Version:', pd.__version__)\n",
    "\n",
    "import sklearn; print('Scikit-Learn Version:', sklearn.__version__)\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import dask_xgboost; print('Dask XGBoost Version:', dask_xgboost.__version__)\n",
    "import dask_cudf\n",
    "\n",
    "import time \n",
    "\n",
    "\n",
    "import rapids_lib_v8 as rl\n",
    "''' NOTE: anytime changes are made to rapids_lib.py you can either:\n",
    "      1. refresh/reload via the code below, OR\n",
    "      2. restart the kernel '''\n",
    "import importlib; importlib.reload(rl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r expLog\n",
    "%store -r trainData_pDF\n",
    "%store -r trainLabels_pDF \n",
    "%store -r testData_pDF\n",
    "%store -r testLabels_pDF\n",
    "\n",
    "%store -r expLog\n",
    "%store -r trainData_cDF\n",
    "%store -r trainLabels_cDF \n",
    "%store -r testData_cDF\n",
    "%store -r testLabels_cDF\n",
    "\n",
    "nCores = !nproc --all\n",
    "nCores = int(nCores[0]) # we want to extract number of cores the CPU has \n",
    "\n",
    "paramsCPU = {\n",
    "    'max_depth': 10,\n",
    "    'learning_rate': .1,\n",
    "    'num_boost_rounds': 100,\n",
    "    'lambda': 1,\n",
    "    'objective': 'binary:hinge',\n",
    "    'tree_method': 'hist',\n",
    "    'n_jobs': nCores,\n",
    "    'random_state': 0\n",
    "}\n",
    "\n",
    "paramsGPU = {\n",
    "    'max_depth': 10,\n",
    "    'learning_rate': .1,\n",
    "    'num_boost_rounds': 100,\n",
    "    'lambda': 1,\n",
    "    'objective': 'binary:hinge',\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'n_gpus': 1,    \n",
    "    'random_state': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_partitions = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " #create Dask cuDF dataframes\n",
    "trainData_dask_cDF = dask_cudf.from_cudf(trainData_cDF, npartitions = num_partitions)\n",
    "testData_dask_cDF = dask_cudf.from_cudf(testData_cDF, npartitions = num_partitions)\n",
    "trainLabels_dask_cDF = dask_cudf.from_cudf(trainLabels_cDF, npartitions = num_partitions)\n",
    "testLabels_dask_cDF = dask_cudf.from_cudf(testLabels_cDF, npartitions = num_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dask_cudf.core.DataFrame"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(testLabels_dask_cDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Dask DataFrames\n",
    "trainData_dask_pDF = dask.dataframe.from_pandas(trainData_pDF, npartitions = num_partitions)\n",
    "testData_dask_pDF = dask.dataframe.from_pandas(testData_pDF, npartitions = num_partitions)\n",
    "trainLabels_dask_pDF = dask.dataframe.from_pandas(trainLabels_pDF, npartitions=num_partitions)\n",
    "testLabels_dask_pDF=dask.dataframe.from_pandas(testLabels_pDF,npartitions = num_partitions)\n",
    "\n",
    "# #create Dask cuDF dataframes\n",
    "# trainData_dask_cDF = dask_cudf.from_dask_dataframe(trainData_dask_pDF)\n",
    "# testData_dask_cDF = dask_cudf.from_dask_dataframe(testData_dask_pDF)\n",
    "# trainLabels_dask_cDF = dask_cudf.from_dask_dataframe(trainLabels_dask_pDF)\n",
    "# testLabels_dask_cDF = dask_cudf.from_dask_dataframe(testLabels_dask_pDF)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function train in module dask_xgboost.core:\n",
      "\n",
      "train(client, params, data, labels, dmatrix_kwargs={}, **kwargs)\n",
      "    Train an XGBoost model on a Dask Cluster\n",
      "    \n",
      "    This starts XGBoost on all Dask workers, moves input data to those workers,\n",
      "    and then calls ``xgboost.train`` on the inputs.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    client: dask.distributed.Client\n",
      "    params: dict\n",
      "        Parameters to give to XGBoost (see xgb.Booster.train)\n",
      "    data: dask array or dask.dataframe\n",
      "    labels: dask.array or dask.dataframe\n",
      "    dmatrix_kwargs: Keywords to give to Xgboost DMatrix\n",
      "    **kwargs: Keywords to give to XGBoost train\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> client = Client('scheduler-address:8786')  # doctest: +SKIP\n",
      "    >>> data = dd.read_csv('s3://...')  # doctest: +SKIP\n",
      "    >>> labels = data['outcome']  # doctest: +SKIP\n",
      "    >>> del data['outcome']  # doctest: +SKIP\n",
      "    >>> train(client, params, data, labels, **normal_kwargs)  # doctest: +SKIP\n",
      "    <xgboost.core.Booster object at ...>\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    predict\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(dask_xgboost.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU training time:36.10901665687561 seconds.\n"
     ]
    }
   ],
   "source": [
    "''' -------------------------------------------------------------------------\n",
    ">  CPU Train and Test\n",
    "------------------------------------------------------------------------- '''\n",
    "startTime = time.time()\n",
    "Dask_xgBoostModelCPU = dask_xgboost.train( client=client, \n",
    "                                     params = paramsCPU,\n",
    "                                     data= trainData_dask_pDF, \n",
    "                                     labels = trainLabels_dask_pDF,\n",
    "                                     num_boost_round = paramsCPU['num_boost_rounds'])\n",
    "\n",
    "print(\"CPU training time:\" + str(time.time()-startTime),\"seconds.\")\n",
    "\n",
    "#Generate Predictions\n",
    "predictionsCPU = dask_xgboost.predict(client, Dask_xgBoostModelCPU, testData_dask_pDF)\n",
    "predictionsCPU_con = dask.dataframe.multi.concat([predictionsCPU], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU training time:8.091978549957275 seconds.\n"
     ]
    }
   ],
   "source": [
    "''' -------------------------------------------------------------------------\n",
    ">  GPU Train and Test\n",
    "------------------------------------------------------------------------- '''\n",
    "startTime = time.time()\n",
    "Dask_xgBoostModelGPU = dask_xgboost.train( client=client, \n",
    "                                     params = paramsGPU,\n",
    "                                     data= trainData_dask_pDF, \n",
    "                                     labels = trainLabels_dask_pDF,\n",
    "                                     num_boost_round = paramsGPU['num_boost_rounds'])\n",
    "\n",
    "print(\"GPU training time:\" + str(time.time()-startTime),\"seconds.\")\n",
    "\n",
    "# #Generate Predictions\n",
    "predictionsGPU = dask_xgboost.predict(client, Dask_xgBoostModelGPU, testData_dask_pDF)\n",
    "predictionsGPU_con = dask.dataframe.multi.concat([predictionsGPU], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(predictionsGPU_con.compute())\n",
    "# print(testLabels_dask_pDF.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9429875\n"
     ]
    }
   ],
   "source": [
    "gpu_accuracy = accuracy_score(testLabels_dask_pDF.astype(int),predictionsGPU_con.compute());print(gpu_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9476625\n"
     ]
    }
   ],
   "source": [
    "cpu_accuracy = accuracy_score(testLabels_dask_pDF.astype(int),predictionsCPU_con.compute());print(cpu_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
