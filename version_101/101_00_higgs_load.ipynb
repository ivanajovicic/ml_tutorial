{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation with RAPIDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will learn how to easy generate syntetic dataset and do the basic manipulation to use it for training. \n",
    "\n",
    "**Table of Contents**\n",
    "* 1. [Import Libraries](#Import-Libraries)\n",
    "* 2. [Data Loading](#Data-Loading)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Built in Python imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from enum import Enum\n",
    "\n",
    "if sys.version_info[0] >= 3:\n",
    "    from urllib.request import urlretrieve  # pylint: disable=import-error,no-name-in-module\n",
    "else:\n",
    "    from urllib import urlretrieve  # pylint: disable=import-error,no-name-in-module\n",
    "\n",
    "class LearningTask(Enum):\n",
    "    REGRESSION = 1\n",
    "    CLASSIFICATION = 2\n",
    "    MULTICLASS_CLASSIFICATION = 3\n",
    "\n",
    "class Data:  # pylint: disable=too-few-public-methods,too-many-arguments\n",
    "    def __init__(self, X_train, X_test, y_train, y_test, learning_task, qid_train=None,\n",
    "                 qid_test=None):\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.learning_task = learning_task\n",
    "        # For ranking task\n",
    "        self.qid_train = qid_train\n",
    "        self.qid_test = qid_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional CPU imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy Version: 1.16.2\n",
      "pandas Version: 0.24.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np;print('numpy Version:', np.__version__)\n",
    "import pandas as pd;print('pandas Version:', pd.__version__)\n",
    "import sklearn\n",
    "import pickle\n",
    "import numpy as np; import numpy.matlib\n",
    "## Visulaization libraries \n",
    "import ipyvolume as ipv\n",
    "import matplotlib.pyplot as plt \n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Algorithms and Dataset libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports for GPU dataset and algorithms accelerations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cupy Version 6.2.0\n",
      "cudf Version 0.9.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'rapids_lib_v10' from '/rapids/notebooks/ml_tutorial/version_101/rapids_lib_v10.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cupy;print('cupy Version', cupy.__version__)\n",
    "import cudf;print('cudf Version', cudf.__version__)\n",
    "\n",
    "\n",
    "import rapids_lib_v10 as rl\n",
    "''' NOTE: anytime changes are made to rapids_lib.py you can either:\n",
    "      1. refresh/reload via the code below, OR\n",
    "      2. restart the kernel '''\n",
    "import importlib; importlib.reload(rl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will load the [Higgs Boson detection data](https://archive.ics.uci.edu/ml/datasets/HIGGS). The data has been produced using Monte Carlo simulations. The first 21 features (columns 2-22) are kinematic properties measured by the particle detectors in the accelerator. The last seven features are functions of the first 21 features; these are high-level features derived by physicists to help discriminate between the two classes. There is an interest in using deep learning methods to obviate the need for physicists to manually develop such features.\n",
    "**Attribute Information**\n",
    "- The first column is the class label (1 for signal, 0 for background)\n",
    "- 21 low-level features (kinematic properties): lepton pT, lepton eta, lepton phi, missing energy magnitude, missing energy phi, jet 1 pt, jet 1 eta, jet 1 phi, jet 1 b-tag, jet 2 pt, jet 2 eta, jet 2 phi, jet 2 b-tag, jet 3 pt, jet 3 eta, jet 3 phi, jet 3 b-tag, jet 4 pt, jet 4 eta, jet 4 phi, jet 4 b-tag\n",
    "- 7 high-level features derived by physicists: m_jj, m_jjj, m_lv, m_jlv, m_bb, m_wbb, m_wwbb.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the function below to get the data and split it to training and testing parts. Here we are using 80% of our data for training. You can easily adjust this as well as  how many of rows (out of 11 milions total) you want to read. This will be usefull for the testing purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_higgs(dataset_folder, nrows):\n",
    "    url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz'\n",
    "    local_url = os.path.join(dataset_folder, os.path.basename(url))\n",
    "    pickle_url = os.path.join(dataset_folder,\n",
    "                              \"higgs\" + (\"\" if nrows is None else \"-\" + str(nrows)) + \".pkl\")\n",
    "\n",
    "    if os.path.exists(pickle_url):\n",
    "        return pickle.load(open(pickle_url, \"rb\"))\n",
    "\n",
    "    if not os.path.isfile(local_url):\n",
    "        urlretrieve(url, local_url)\n",
    "    higgs = pd.read_csv(local_url, nrows=nrows, header = None, \n",
    "                        names= ['label','lepton_pT','lepton_eta','lepton_phi','missing_energy_magnitude','missing_energy_phi','jet_1_pt',\n",
    "                             'jet_1_eta','jet_1_phi','jet_1_b_tag','jet_2_pt','jet_2_eta','jet_2_phi','jet_2_b_tag',\n",
    "                             'jet_3_pt','jet_3_eta','jet_3_phi','jet_3_b-tag','jet_4_pt','jet_4_eta','jet_4_phi',\n",
    "                             'jet_4_b_tag','m_jj','m_jjj','m_lv','m_jlv','m_bb','m_wbb','m_wwbb'] )\n",
    "    X = higgs.iloc[:, 1:]\n",
    "    y = higgs.iloc[:, 0]\n",
    "    \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=77,\n",
    "                                                        test_size=0.2,\n",
    "                                                        )\n",
    "    data = Data(X_train, X_test, y_train, y_test, LearningTask.CLASSIFICATION)\n",
    "    pickle.dump(data, open(pickle_url, \"wb\"), protocol=4)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first read a very small subset of this dataset. It will take few minutes. While waiting you can open a new jupyter lab window and set up your GPU Dashboard for the monitoring purposes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 11000000\n",
    "higgs_df = prepare_higgs(\".\", num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in training dataset: 8800000\n",
      "Number of rows in testing dataset: 2200000\n"
     ]
    }
   ],
   "source": [
    "print('Number of rows in training dataset:', len(higgs_df.X_train))\n",
    "print('Number of rows in testing dataset:',len(higgs_df.X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(higgs_df.X_train))\n",
    "print(type(higgs_df.y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will convert the data object to 2 formats:\n",
    "\n",
    "  1. **Pandas DataFrame** for  CPU based tasks \n",
    "  2. **cuDF Dataframe** objects for GPU tasks \n",
    "  \n",
    "[Rapids cuDF](https://github.com/rapidsai/cudf) is a GPU Data Frames library for loading, joinign,aggregating, filtering, and optherwise manipulating data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas dataframe objects \n",
    "trainData_pDF = higgs_df.X_train\n",
    "testData_pDF = higgs_df.X_test\n",
    "trainLabels_pDF = higgs_df.y_train.to_frame()\n",
    "testLabels_pDF = higgs_df.y_test.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cuDF dataframe objects \n",
    "trainData_cDF = cudf.DataFrame.from_pandas(trainData_pDF)\n",
    "testData_cDF = cudf.DataFrame.from_pandas(testData_pDF)\n",
    "trainLabels_cDF = cudf.DataFrame.from_pandas(trainLabels_pDF)\n",
    "testLabels_cDF = cudf.DataFrame.from_pandas(testLabels_pDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now save those object using jupyter lab magic function `%store` and use it for our next task . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'trainData_cDF' (DataFrame)\n",
      "Stored 'trainLabels_cDF' (DataFrame)\n",
      "Stored 'testData_cDF' (DataFrame)\n",
      "Stored 'testLabels_cDF' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store trainData_cDF\n",
    "%store trainLabels_cDF\n",
    "%store testData_cDF \n",
    "%store testLabels_cDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'trainData_pDF' (DataFrame)\n",
      "Stored 'trainLabels_pDF' (DataFrame)\n",
      "Stored 'testData_pDF' (DataFrame)\n",
      "Stored 'testLabels_pDF' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%store trainData_pDF\n",
    "%store trainLabels_pDF \n",
    "%store testData_pDF\n",
    "%store testLabels_pDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
