{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation with RAPIDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will learn how to easy generate syntetic dataset and do the basic manipulation to use it for training. \n",
    "\n",
    "**Table of Contents**\n",
    "* 1. [Import Libraries](#Import-Libraries)\n",
    "* 2. [Data Generation](#Data-Generation)\n",
    "* 3. [Split Training and Testing Data](#Split-Training-and-Testing-data)\n",
    "* 4. [Rescale/Normalize the Data](#Rescale-/-Normalize-the-data)\n",
    "    * 4.1. [CPU Split and Scale](#CPU-split-&-scale)\n",
    "    * 4.2. [GPU Split and Scale](#GPU-split-&-scale)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Built in Python imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional CPU imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np;print('numpy Version:', np.__version__)\n",
    "import pandas as pd;print('pandas Version:', pd.__version__)\n",
    "import sklearn\n",
    "## Visulaization libraries \n",
    "import ipyvolume as ipv\n",
    "import matplotlib.pyplot as plt \n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Algorithms and Dataset libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets; \n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports for GPU dataset and algorithms accelerations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy;print('cupy Version', cupy.__version__)\n",
    "import cudf;print('cudf Version', cudf.__version__)\n",
    "\n",
    "\n",
    "import rapids_lib_v10 as rl\n",
    "''' NOTE: anytime changes are made to rapids_lib.py you can either:\n",
    "      1. refresh/reload via the code below, OR\n",
    "      2. restart the kernel '''\n",
    "import importlib; importlib.reload(rl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate data shapes [coordinate lists] and hand them to the GPU. The GPU will randomly build 3D blobs [ cupy.random.normal ] around each coordinate point to create a much larger, noisier, and more realistic dataset.\n",
    "\n",
    "Using this concept we offer the following dataset variations:\n",
    "1. Helix - two entwined coils, inspired by DNA casing\n",
    "2. Whirl - an increasingly unwinding Helix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl.plot_dataset_variants()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to generate the dataset with either of two variants, take a look into the library and create your own! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(rl.gen_blob_coils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the set of values we used for our experiment: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nBlobPoints = 1000\n",
    "nCoordinates =400\n",
    "sdevScales = [ .3, .3, .3]\n",
    "noiseScale = 1/10.\n",
    "coilDensity = 12\n",
    "trainTestOverlap = .05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels, t_gen = rl.gen_blob_coils( coilType='helix', \n",
    "                                        shuffleFlag = False, \n",
    "                                         nBlobPoints = nBlobPoints,  \n",
    "                                         nCoordinates = nCoordinates, \n",
    "                                         sdevScales = sdevScales, \n",
    "                                         noiseScale = noiseScale, \n",
    "                                         coilDensity = coilDensity )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This RAPIDS library will return cuDF Dataframe object as expected. Let's check ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Type of dataset rl returned is', type(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Training and Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expLog = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescale / Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataframe_inplace ( targetDF, trainMeans = {}, trainSTDevs = {} ):    \n",
    "    print('rescaling data')\n",
    "    sT = time.time()\n",
    "    for iCol in targetDF.columns:\n",
    "        \n",
    "        # omit scaling label column\n",
    "        if iCol == targetDF.columns[-1] == 'label': continue\n",
    "            \n",
    "        # compute means and standard deviations for each column [ should skip for test data ]\n",
    "        if iCol not in trainMeans.keys() and iCol not in trainSTDevs.keys():            \n",
    "            trainMeans[iCol] = targetDF[iCol].mean()\n",
    "            trainSTDevs[iCol] = targetDF[iCol].std()\n",
    "            \n",
    "        # apply scaling to each column\n",
    "        targetDF[iCol] = ( targetDF[iCol] - trainMeans[iCol] ) / ( trainSTDevs[iCol] + 1e-10 )\n",
    "        \n",
    "    return trainMeans, trainSTDevs, time.time() - sT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU vs CPU work \n",
    "\n",
    "We will use two above helper functions to split our dataset into training and testing and normalize it afterwards.\n",
    "Before we dive deep into the work I want to make a note about variable naming:\n",
    "\n",
    "We will use **_pDF** (as Pandas DataFrame) sufix to our varaible names to emphasize that the variable resides on CPU. Also , we will use **_cDF** (cudf DataFrame) to recognize the variables that are using GPU acclerated libraries (cudf and cuml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU split & scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to have our functions using CPU only we will use use pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split\n",
    "trainData_pDF, trainLabels_pDF, testData_pDF, testLabels_pDF, t_split_CPU = rl.split_train_test_nfolds( \n",
    "    data.to_pandas(),\n",
    "    labels.to_pandas(),\n",
    "    trainTestOverlap = trainTestOverlap )\n",
    "\n",
    "# apply standard scaling\n",
    "trainMeans_CPU, trainSTDevs_CPU, t_scaleTrain_CPU = scale_dataframe_inplace ( trainData_pDF )\n",
    "_,_, t_scaleTest_CPU = scale_dataframe_inplace ( testData_pDF, trainMeans_CPU, trainSTDevs_CPU )    \n",
    "\n",
    "expLog = rl.update_log( expLog, [['CPU_split_train_test', t_split_CPU],\n",
    "                                 ['CPU_scale_train_data', t_scaleTrain_CPU], \n",
    "                                 ['CPU_scale_test_data', t_scaleTest_CPU]] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now take a look into our dataset, shape and datatypes : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testData_pDF.head());\n",
    "print (trainData_pDF.shape)\n",
    "print('trainData_pDF: ', trainData_pDF.shape, type(trainData_pDF), 'trainLabels_pDF: ', trainLabels_pDF.shape, type(trainLabels_pDF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU split & scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split\n",
    "trainData_cDF, trainLabels_cDF, testData_cDF, testLabels_cDF, t_split = rl.split_train_test_nfolds ( \n",
    "    data, \n",
    "    labels, \n",
    "    trainTestOverlap = trainTestOverlap)\n",
    "\n",
    "# apply standard scaling\n",
    "trainMeans, trainSTDevs, t_scaleTrain = scale_dataframe_inplace ( trainData_cDF )\n",
    "_,_, t_scaleTest = scale_dataframe_inplace ( testData_cDF, trainMeans, trainSTDevs )    \n",
    "\n",
    "expLog = rl.update_log( expLog, [['GPU_split_train_test', t_split],\n",
    "                                 ['GPU_scale_train_data', t_scaleTrain],\n",
    "                                 ['GPU_scale_test_data', t_scaleTest]] ); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainData_cDF.head());\n",
    "print (trainData_cDF.shape)\n",
    "print('trainData_cDF: ', trainData_cDF.shape, type(trainData_cDF), 'trainLabels_cDF: ', trainLabels_cDF.shape, type(trainLabels_cDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl.plot_iid_breaking ( trainData_cDF, testData_cDF )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl.plot_train_test(trainData_cDF, trainLabels_cDF, testData_cDF, testLabels_cDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store trainData_cDF\n",
    "%store trainLabels_cDF\n",
    "%store testData_cDF \n",
    "%store testLabels_cDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store expLog\n",
    "%store trainData_pDF\n",
    "%store trainLabels_pDF \n",
    "%store testData_pDF\n",
    "%store testLabels_pDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
