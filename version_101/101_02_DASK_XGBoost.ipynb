{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DASK XGBoost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction to Dask and Dask cuDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Dask](https://dask.org/) is a Python library for parallel computing. In Dask programming, we create computational graphs that define code we **would like** to execute, and then, give these computational graphs to a Dask scheduler which evaluates them lazily, and efficiently, in parallel. In addition to using multiple CPU cores or threads to execute computational graphs in parallel, Dask schedulers can also be configured to execute computational graphs on multiple CPUs, or, as we will do in this workshop, multiple GPUs. On account of its ability to utilize multiple compute resources, Dask programming facilitates operating on datasets that are larger than the memory of a single compute resource.\n",
    "\n",
    "[Dask cuDF](https://github.com/rapidsai/dask-cudf) can be used to distribute dataframe operations on larger-than-memory datasets to multiple GPUs. In this notebook you'll receive and introduction to some key Dask concepts, learn how to setup a Dask cluster for utilizing multiple GPUs, and how to perform simple dataframe operations on distributed Dask dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Dask "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by starting a Dask scheduler which will take care to distribute our work across the 4 available GPUs. In order to do this we need to start a `LocalCUDACluster` instance, using our host machine's IP, and then instantiate a client that can communicate with the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask import delayed\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.6/site-packages/distributed/dashboard/core.py:72: UserWarning: \n",
      "Port 8787 is already in use. \n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the diagnostics dashboard on a random port instead.\n",
      "  warnings.warn(\"\\n\" + msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://192.168.99.2:40069</li>\n",
       "  <li><b>Dashboard: </b><a href='http://192.168.99.2:33697/status' target='_blank'>http://192.168.99.2:33697/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>270.39 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://192.168.99.2:40069' processes=4 cores=4>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster = LocalCUDACluster(ip=\"\")\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dask Dashboard \n",
    "As you can see, the `client` instance gives us information about our CUDA cluster (utilizing 4 GPUs), as well as information about our client connection. Dask ships with an incredibly helpful dashboard, which you can see runs on port `8787`. Open a new browser tab now at `<YOUR_IP_ADDRESS>:8787`, for example `ec2-12-345-67-890.us-east-2.compute.amazonaws.com:8787`, which should open the Dask dashboard, currently idle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy Version: 1.16.4\n",
      "pandas Version: 0.24.2\n",
      "Scikit-Learn Version: 0.21.2\n",
      "Dask XGBoost Version: 0.1.7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'rapids_lib_v8' from '/rapids/notebooks/ml_tutorial/testing/rapids_lib_v8.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install dask_xgboost\n",
    "import numpy as np; print('numpy Version:', np.__version__)\n",
    "import pandas as pd; print('pandas Version:', pd.__version__)\n",
    "import sklearn; print('Scikit-Learn Version:', sklearn.__version__)\n",
    "import dask_xgboost; print('Dask XGBoost Version:', dask_xgboost.__version__)\n",
    "import dask_cudf\n",
    "\n",
    "import time \n",
    "\n",
    "\n",
    "import rapids_lib_v8 as rl\n",
    "''' NOTE: anytime changes are made to rapids_lib.py you can either:\n",
    "      1. refresh/reload via the code below, OR\n",
    "      2. restart the kernel '''\n",
    "import importlib; importlib.reload(rl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r expLog\n",
    "%store -r trainData_pDF\n",
    "%store -r trainLabels_pDF \n",
    "%store -r testData_pDF\n",
    "%store -r testLabels_pDF\n",
    "\n",
    "nCores = !nproc --all\n",
    "nCores = int(nCores[0]) # we want to extract number of cores the CPU has \n",
    "\n",
    "paramsCPU = {\n",
    "    'max_depth': 10,\n",
    "    'learning_rate': .1,\n",
    "    'num_boost_rounds': 100,\n",
    "    'lambda': 1,\n",
    "    'objective': 'binary:hinge',\n",
    "    'tree_method': 'hist',\n",
    "    'n_jobs': nCores,\n",
    "    'random_state': 0\n",
    "}\n",
    "\n",
    "paramsGPU = {\n",
    "    'max_depth': 10,\n",
    "    'learning_rate': .1,\n",
    "    'num_boost_rounds': 100,\n",
    "    'lambda': 1,\n",
    "    'objective': 'binary:hinge',\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'n_gpus': 1,    \n",
    "    'random_state': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' -------------------------------------------------------------------------\n",
    ">  CPU Train and Test\n",
    "------------------------------------------------------------------------- '''\n",
    "def train_model_CPU (DaskClient, trainData_pDF, testData_pDF, paramsCPU = {}):    \n",
    "    print('training xgboost model on {} CPU cores'.format(nCores) )\n",
    "     \n",
    "    \"we will pass the Pandas DataFrame to DMatrix\"\n",
    "#     trainDMatrix = dask_xgboost.DMatrix( trainData_pDF, label = trainLabels_pDF)\n",
    "    startTime = time.time()\n",
    "    xgBoostModelCPU = dask_xgboost.train( dtrain = trainData_pDF, params = paramsCPU,\n",
    "                                   num_boost_round = paramsCPU['num_boost_rounds'])\n",
    "\n",
    "    print(\"CPU training time:\" + str(time.time()-startTime),\"seconds.\")\n",
    "    return xgBoostModelCPU, time.time() - startTime\n",
    "\n",
    "def test_model_CPU ( DaskClient, trainedModelCPU, testData_pDF, testLabels_pDF ):\n",
    "    print('testing xgboost model on CPU')\n",
    "#     testDMatrix = dask_xgboost.DMatrix( testData_pDF, label=testLabels_pDF)\n",
    "    startTime = time.time()    \n",
    "\n",
    "    predictionsCPU = trainedModelCPU.predict(testData_pDF)\n",
    "    print(\"CPU testing time:\" + str(time.time()-startTime), \"seconds.\")\n",
    "    return predictionsCPU, time.time() - startTime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training xgboost model on 40 CPU cores\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "train() missing 3 required positional arguments: 'client', 'data', and 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5088057c06ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#model training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainedModelCPU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_trainCPU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model_CPU\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainData_pDF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainLabels_pDF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparamsCPU\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#model inferencing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpredictionsCPU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_inferCPU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model_CPU\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainedModelCPU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestData_pDF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestLabels_pDF\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-c7548e780113>\u001b[0m in \u001b[0;36mtrain_model_CPU\u001b[0;34m(DaskClient, trainData_pDF, testData_pDF, paramsCPU)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mstartTime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     xgBoostModelCPU = dask_xgboost.train( dtrain = trainData_pDF, params = paramsCPU,\n\u001b[0;32m---> 11\u001b[0;31m                                    num_boost_round = paramsCPU['num_boost_rounds'])\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CPU training time:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstartTime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"seconds.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: train() missing 3 required positional arguments: 'client', 'data', and 'labels'"
     ]
    }
   ],
   "source": [
    "#model training\n",
    "trainedModelCPU, t_trainCPU = train_model_CPU ( client,trainData_pDF, trainLabels_pDF, paramsCPU )\n",
    "#model inferencing\n",
    "predictionsCPU, t_inferCPU = test_model_CPU ( client,trainedModelCPU, testData_pDF, testLabels_pDF )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
