{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost with RAPIDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will show how to work with GPU accelerated XGBoost in RAPIDS with the dataset we just created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of Contents**\n",
    "\n",
    "* [Import Libraries](#Import-Libraries)\n",
    "* [XGBoost Training](#XGBoost-Training)\n",
    "    * [Define Parameters](#Define-Parameters)\n",
    "    * [Train XGBoost Model](#Train-XGBoost-Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy Version: 1.16.2\n",
      "pandas Version: 0.24.2\n",
      "Scikit-Learn Version: 0.20.4\n",
      "XGBoost Version: 0.90.rapidsdev1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'rapids_lib_v8' from '/rapids/notebooks/ml_tutorial/version_101/rapids_lib_v8.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np; print('numpy Version:', np.__version__)\n",
    "import pandas as pd; print('pandas Version:', pd.__version__)\n",
    "import sklearn; print('Scikit-Learn Version:', sklearn.__version__)\n",
    "import xgboost; print('XGBoost Version:', xgboost.__version__)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import time \n",
    "\n",
    "\n",
    "import rapids_lib_v8 as rl\n",
    "''' NOTE: anytime changes are made to rapids_lib.py you can either:\n",
    "      1. refresh/reload via the code below, OR\n",
    "      2. restart the kernel '''\n",
    "import importlib; importlib.reload(rl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook we already created our dataset and made it ready for training and testing .\n",
    "Remember, we intentionally named the objects we created in a way you can easy distingwish whether you are using CPU based libraries such as pandasDataFrame(`_pDF`) vs GPU library cuDF (`_cDF`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first load the variables we stored in the previous notebook. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r trainData_cDF\n",
    "%store -r trainLabels_cDF\n",
    "%store -r testData_cDF \n",
    "%store -r testLabels_cDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r expLog\n",
    "%store -r trainData_pDF\n",
    "%store -r trainLabels_pDF \n",
    "%store -r testData_pDF\n",
    "%store -r testLabels_pDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I like to say that we have  three main components here:\n",
    "\n",
    "<font color=green>__1. Data to DMatrix format__</font>\n",
    "\n",
    "\n",
    "In order for XGBoost to be able to use our data, we’ll need to transform it into a specific format that XGBoost can handle. That format is called **DMatrix**. It’s a very simple one-linear to transform a numpy array of data to DMatrix format.\n",
    "\n",
    "We can instantiate an object of the `xgboost.DMatrix` by passing in the feature matrix as the first argument followed by the label vector.\n",
    "\n",
    "`D_train = xgboost.DMatrix(X_train, label=Y_train)\n",
    " D_test = xgboost.DMatrix(X_test, label=Y_test)`\n",
    " \n",
    "<font color=green>__2. Defining the parameters__</font> \n",
    "\n",
    "Now that our data is all loaded up, we can define the **parameters**. We’ve set up some of the most important ones below to get us started. For more complicated tasks and models, the full list of possible parameters is available on the official [XGBoost website](https://xgboost.readthedocs.io/en/latest/parameter.html)\n",
    "\n",
    "<font color=green>__3. Training and testing__<font/>\n",
    "\n",
    "We can finally train our model:\n",
    "`model = xgboost.train(param, D_train, steps)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's separate the parameters we are using when training CPU vs GPU based XGboost. Take some time to see that almost everything is the same besides 2 parameters:\n",
    "\n",
    "- `tree_method` : we will use `gpu_hist` for GPU training\n",
    "- `n_gpus vs n_jobs` : instead of specifying how many CPU cores our training will use, for GPU based XGboost we will specify number of GPUs we are using. \n",
    "\n",
    "\"`-1`\" means *use all the avaialble GPUs you have*\n",
    "\n",
    "Note: *Hinge loss* penalizes incorrectly classified examples and correctly classified examples that lie within the margin(correct but not confident)\n",
    "Hinge Lost is faster to train via gradient decent, easier to compute than cross entropy (log loss).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nCores = !nproc --all\n",
    "nCores = int(nCores[0]) # we want to extract number of cores the CPU has \n",
    "\n",
    "paramsCPU = {\n",
    "    'max_depth': 10,\n",
    "    'learning_rate': .1,\n",
    "    'num_boost_rounds': 100,\n",
    "    'lambda': 1,\n",
    "    'objective': 'binary:hinge',\n",
    "    'tree_method': 'hist',\n",
    "    'n_jobs': nCores,\n",
    "    'random_state': 0\n",
    "}\n",
    "\n",
    "paramsGPU = {\n",
    "    'max_depth': 10,\n",
    "    'learning_rate': .1,\n",
    "    'num_boost_rounds': 100,\n",
    "    'lambda': 1,\n",
    "    'objective': 'binary:hinge',\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'n_gpus': 1,    \n",
    "    'random_state': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train XGBoost Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Monitor gpu utilization** by typing `watch 0.1 nvidia-smi` in your console or use our GPU monitoring tool in System Dashboards. \n",
    "You can find it on the left side of our notebook. I recommend to organize your Dashboard in a separate window to look something like this :\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![System Monitoring Tool](monitoring.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we have our parameters defined, let's use two functions below to:\n",
    "1. pass the parameters\n",
    "2.convert data to Dmatrix and\n",
    "3.complete the training and testing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that win both cases (GPU and CPU) we need to pass pandas dataframe to DMatrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' -------------------------------------------------------------------------\n",
    ">  CPU Train and Test\n",
    "------------------------------------------------------------------------- '''\n",
    "def train_model_CPU (trainData_pDF, testData_pDF, paramsCPU = {}):    \n",
    "    print('training xgboost model on {} CPU cores'.format(nCores) )\n",
    "    \n",
    "    \"we will pass the Pandas DataFrame to DMatrix\"\n",
    "    trainDMatrix = xgboost.DMatrix( trainData_pDF, label = trainLabels_pDF)\n",
    "    startTime = time.time()\n",
    "    xgBoostModelCPU = xgboost.train( dtrain = trainDMatrix, params = paramsCPU,\n",
    "                                   num_boost_round = paramsCPU['num_boost_rounds'])\n",
    "    cpuTrainingTime = time.time()-startTime\n",
    "    print(\"CPU training time:\" , cpuTrainingTime,\"seconds.\")\n",
    "    return xgBoostModelCPU, cpuTrainingTime\n",
    "\n",
    "def test_model_CPU ( trainedModelCPU, testData_pDF, testLabels_pDF ):\n",
    "    print('testing xgboost model on CPU')\n",
    "    testDMatrix = xgboost.DMatrix( testData_pDF, label=testLabels_pDF)\n",
    "    startTime = time.time()    \n",
    "\n",
    "    predictionsCPU = trainedModelCPU.predict(testDMatrix)\n",
    "    cpuTestingTime = time.time()-startTime\n",
    "    print(\"CPU testing time:\" ,cpuTestingTime, \"seconds.\")\n",
    "    return predictionsCPU, cpuTestingTime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training xgboost model on 40 CPU cores\n",
      "CPU training time: 915.9243884086609 seconds.\n",
      "testing xgboost model on CPU\n",
      "CPU testing time: 0.05548715591430664 seconds.\n"
     ]
    }
   ],
   "source": [
    "#model training\n",
    "trainedModelCPU, t_trainCPU = train_model_CPU ( trainData_pDF, trainLabels_pDF, paramsCPU )\n",
    "#model inferencing\n",
    "predictionsCPU, t_inferCPU = test_model_CPU ( trainedModelCPU, testData_pDF, testLabels_pDF )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " + adding log entry [ CPU_model_training       : 915.92439 s ]\n",
      " + adding log entry [ CPU_model_inference      :   0.05549 s ]\n"
     ]
    }
   ],
   "source": [
    "expLog = rl.update_log( expLog, [['CPU_model_training', t_trainCPU], ['CPU_model_inference', t_inferCPU]] ); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' -------------------------------------------------------------------------\n",
    ">  GPU Train and Test\n",
    "------------------------------------------------------------------------- '''\n",
    "def train_model_GPU (trainData_cDF, testData_cDF, paramsGPU = {}):    \n",
    "    print('training xgboost model on GPU');  \n",
    "    startTime = time.time() \n",
    "    trainDMatrix = xgboost.DMatrix( trainData_cDF, label = trainLabels_cDF)\n",
    "    \n",
    "    trainedModelGPU = xgboost.train( dtrain = trainDMatrix, params = paramsGPU, num_boost_round = paramsGPU['num_boost_rounds'] )\n",
    "    gpuTrainingTime = time.time()-startTime\n",
    "    \n",
    "    print(\"GPU training time:\" ,gpuTrainingTime,\"seconds\")\n",
    "    return trainedModelGPU, gpuTrainingTime\n",
    "\n",
    "def test_model_GPU ( trainedModelGPU, testData_cDF, testLabels_cDF ):\n",
    "    print('testing xgboost model on GPU')\n",
    "    startTime = time.time()   \n",
    "    testDMatrix = xgboost.DMatrix( testData_cDF, label=testLabels_cDF)\n",
    "    \n",
    "    predictionsGPU = trainedModelGPU.predict(testDMatrix)\n",
    "    gpuTestingTime = time.time()-startTime\n",
    "    print(\"GPU testing time:\" ,gpuTestingTime, \"seconds\")\n",
    "    return predictionsGPU, gpuTestingTime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training xgboost model on GPU\n",
      "GPU training time: 2.4469590187072754 seconds\n",
      "testing xgboost model on GPU\n",
      "GPU testing time: 0.005692958831787109 seconds\n"
     ]
    }
   ],
   "source": [
    "#model training\n",
    "trainedModelGPU, t_trainGPU = train_model_GPU ( trainData_cDF, trainLabels_cDF, paramsGPU )\n",
    "# model inferencing\n",
    "predictionsGPU, t_inferGPU = test_model_GPU ( trainedModelGPU, testData_cDF, testLabels_cDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9433875\n"
     ]
    }
   ],
   "source": [
    "gpu_accuracy = accuracy_score(testLabels_pDF,predictionsGPU);print(gpu_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9466375\n"
     ]
    }
   ],
   "source": [
    "cpu_accuracy = accuracy_score(testLabels_pDF,predictionsCPU);print(cpu_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'trainData_cDF' (DataFrame)\n",
      "Stored 'trainLabels_cDF' (DataFrame)\n",
      "Stored 'testData_cDF' (DataFrame)\n",
      "Stored 'testLabels_cDF' (DataFrame)\n",
      "Stored 'expLog' (dict)\n",
      "Stored 'trainData_pDF' (DataFrame)\n",
      "Stored 'trainLabels_pDF' (DataFrame)\n",
      "Stored 'testData_pDF' (DataFrame)\n",
      "Stored 'testLabels_pDF' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store trainData_cDF\n",
    "%store trainLabels_cDF\n",
    "%store testData_cDF \n",
    "%store testLabels_cDF\n",
    "\n",
    "%store expLog\n",
    "%store trainData_pDF\n",
    "%store trainLabels_pDF \n",
    "%store testData_pDF\n",
    "%store testLabels_pDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
