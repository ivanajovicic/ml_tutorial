{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width = 85% src='rapids_motivation.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width = 75% src='choices.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview \n",
    "1 -- synthetically generate a learning problem [ classification ]\n",
    "\n",
    "2 -- go through a data science pipeline [ pre-processing , splitting , viz ]\n",
    "\n",
    "3 -- model building [ xgboost ]\n",
    "> hyper-parameters [ max-depth, nTrees, learning rate, regularization ... ]\n",
    "\n",
    "> demonstrate performance [ CPU vs 1 GPU ]\n",
    "\n",
    "4 -- scaling and hyper-parameter search\n",
    "> dask + rapids [ xgboost ]\n",
    "    \n",
    "5 -- visualize search and reveal best model parameters\n",
    "\n",
    "TODO - generate figures that caputre benefit of GPU scaling\n",
    "\n",
    "6 -- extensions[ multi-node [ dask kubernetes ], dask_xgboost [ larger dataset ] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from enum import Enum\n",
    "\n",
    "if sys.version_info[0] >= 3:\n",
    "    from urllib.request import urlretrieve  # pylint: disable=import-error,no-name-in-module\n",
    "else:\n",
    "    from urllib import urlretrieve  # pylint: disable=import-error,no-name-in-module\n",
    "\n",
    "class LearningTask(Enum):\n",
    "    REGRESSION = 1\n",
    "    CLASSIFICATION = 2\n",
    "    MULTICLASS_CLASSIFICATION = 3\n",
    "\n",
    "class Data:  # pylint: disable=too-few-public-methods,too-many-arguments\n",
    "    def __init__(self, X_train, X_test, y_train, y_test, learning_task, qid_train=None,\n",
    "                 qid_test=None):\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.learning_task = learning_task\n",
    "        # For ranking task\n",
    "        self.qid_train = qid_train\n",
    "        self.qid_test = qid_test\n",
    " \n",
    "      \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np; import numpy.matlib\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np; import numpy.matlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyvolume as ipv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algos, datasets\n",
    "import xgboost; from xgboost import plot_tree\n",
    "from sklearn import datasets; from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy\n",
    "import cudf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rapids_lib_v10 as rl\n",
    "''' NOTE: anytime changes are made to rapids_lib.py you can either:\n",
    "      1. refresh/reload via the code below, OR\n",
    "      2. restart the kernel '''\n",
    "import importlib; importlib.reload(rl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data retrieving\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_higgs(dataset_folder, nrows):\n",
    "    url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz'\n",
    "    local_url = os.path.join(dataset_folder, os.path.basename(url))\n",
    "    pickle_url = os.path.join(dataset_folder,\n",
    "                              \"higgs\" + (\"\" if nrows is None else \"-\" + str(nrows)) + \".pkl\")\n",
    "\n",
    "    if os.path.exists(pickle_url):\n",
    "        return pickle.load(open(pickle_url, \"rb\"))\n",
    "\n",
    "    if not os.path.isfile(local_url):\n",
    "        urlretrieve(url, local_url)\n",
    "    higgs = pd.read_csv(local_url, nrows=nrows, header = None, \n",
    "                        names= ['label','lepton_pT','lepton_eta','lepton_phi','missing_energy_magnitude','missing_energy_phi','jet_1_pt',\n",
    "                             'jet_1_eta','jet_1_phi','jet_1_b_tag','jet_2_pt','jet_2_eta','jet_2_phi','jet_2_b_tag',\n",
    "                             'jet_3_pt','jet_3_eta','jet_3_phi','jet_3_b-tag','jet_4_pt','jet_4_eta','jet_4_phi',\n",
    "                             'jet_4_b_tag','m_jj','m_jjj','m_lv','m_jlv','m_bb','m_wbb','m_wwbb'] )\n",
    "    X = higgs.iloc[:, 1:]\n",
    "    y = higgs.iloc[:, 0]\n",
    "    \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=77,\n",
    "                                                        test_size=0.2,\n",
    "                                                        )\n",
    "    data = Data(X_train, X_test, y_train, y_test, LearningTask.CLASSIFICATION)\n",
    "    pickle.dump(data, open(pickle_url, \"wb\"), protocol=4)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 11000000\n",
    "higgs_df = prepare_higgs(\".\", num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of rows in training dataset:', len(higgs_df.X_train))\n",
    "print('Number of rows in testing dataset:',len(higgs_df.X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helix and Whirl Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData_pDF = higgs_df.X_train\n",
    "testData_pDF = higgs_df.X_test\n",
    "trainLabels_pDF = higgs_df.y_train.to_frame()\n",
    "testLabels_pDF = higgs_df.y_test.to_frame()\n",
    "\n",
    "trainData_cDF = cudf.DataFrame.from_pandas(trainData_pDF)\n",
    "testData_cDF = cudf.DataFrame.from_pandas(testData_pDF)\n",
    "trainLabels_cDF = cudf.DataFrame.from_pandas(trainLabels_pDF)\n",
    "testLabels_cDF = cudf.DataFrame.from_pandas(testLabels_pDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train and test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expLog = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluating a Single Model on CPU and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' -------------------------------------------------------------------------\n",
    ">  GPU Train and Test\n",
    "------------------------------------------------------------------------- '''\n",
    "def train_model_GPU (trainData_cDF, testData_cDF, paramsGPU = {}):    \n",
    "    print('training xgboost model on GPU');  \n",
    "    startTime = time.time()    \n",
    "    \n",
    "    trainDMatrix = xgboost.DMatrix( trainData_cDF.to_pandas(), label = trainLabels_cDF.to_pandas())    \n",
    "    trainedModelGPU = xgboost.train( dtrain = trainDMatrix, params = paramsGPU, num_boost_round = paramsGPU['num_boost_rounds'] )\n",
    "    \n",
    "    return trainedModelGPU, time.time() - startTime\n",
    "\n",
    "def test_model_GPU ( trainedModelGPU, testData_cDF, testLabels_cDF ):\n",
    "    print('testing xgboost model on GPU')\n",
    "    startTime = time.time()   \n",
    "    \n",
    "    testDMatrix = xgboost.DMatrix( data = testData_cDF.to_pandas(), label = testLabels_cDF.to_pandas())    \n",
    "    predictionsGPU = trainedModelGPU.predict(testDMatrix)\n",
    "    \n",
    "    return predictionsGPU, time.time() - startTime\n",
    "\n",
    "''' -------------------------------------------------------------------------\n",
    ">  CPU Train and Test\n",
    "------------------------------------------------------------------------- '''\n",
    "def train_model_CPU (trainData_cDF, trainLabels_cDF, paramsCPU = {}):    \n",
    "    print('training xgboost model on {} CPU cores'.format(nCores) )\n",
    "\n",
    "    startTime = time.time()\n",
    "    \n",
    "    trainDMatrix = xgboost.DMatrix( trainData_cDF.to_pandas(), label = trainLabels_cDF.to_pandas())\n",
    "    \n",
    "    xgBoostModelCPU = xgboost.train( dtrain = trainDMatrix, params = paramsCPU, num_boost_round = paramsCPU['num_boost_rounds'])\n",
    "    \n",
    "    return xgBoostModelCPU, time.time() - startTime\n",
    "\n",
    "def test_model_CPU ( trainedModelCPU, testData_cDF, testLabels_cDF ):\n",
    "    print('testing xgboost model on CPU')\n",
    "    startTime = time.time()\n",
    "    \n",
    "    testDMatrix = xgboost.DMatrix( testData_cDF.to_pandas(), label = testLabels_cDF.to_pandas())\n",
    "    predictionsCPU = trainedModelCPU.predict(testDMatrix)\n",
    "    \n",
    "    return predictionsCPU, time.time() - startTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model parameters\n",
    "https://xgboost.readthedocs.io/en/latest/parameter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nCores = !nproc --all\n",
    "nCores = int(nCores[0])\n",
    "\n",
    "paramsCPU = {\n",
    "    'max_depth': 3,\n",
    "    'num_boost_rounds': 100,    \n",
    "    'learning_rate': .1,\n",
    "    'lambda': 1,    \n",
    "    'objective': 'binary:hinge',\n",
    "    'tree_method': 'hist',\n",
    "    'n_jobs': nCores,\n",
    "    'random_state': 0\n",
    "}\n",
    "\n",
    "paramsGPU = {\n",
    "    'max_depth': 3,\n",
    "    'num_boost_rounds': 100,\n",
    "    'learning_rate': .1,\n",
    "    'lambda': 1,    \n",
    "    'objective': 'binary:hinge',\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'n_gpus': 1,    \n",
    "    'random_state': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CPU** Training and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainedModelCPU, t_trainCPU = train_model_CPU ( trainData_cDF, trainLabels_cDF, paramsCPU )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsCPU, t_inferCPU = test_model_CPU ( trainedModelCPU, testData_cDF, testLabels_cDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expLog = rl.update_log( expLog, [['CPU_model_training', t_trainCPU], ['CPU_model_inference', t_inferCPU]] ); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **GPU** Model Training and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainedModelGPU, t_trainGPU = train_model_GPU ( trainData_cDF, trainLabels_cDF, paramsGPU )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsGPU, t_inferGPU = test_model_GPU ( trainedModelGPU, testData_cDF, testLabels_cDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expLog = rl.update_log( expLog, [['GPU_model_training', t_trainGPU], ['GPU_model_inference', t_inferGPU]] ); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare CPU and GPU Accuracy [ plot on train and test ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLabels_pDF = trainLabels_cDF.to_pandas()\n",
    "testLabels_pDF = testLabels_cDF.to_pandas()\n",
    "\n",
    "trainPredictionsCPU, _ = test_model_CPU ( trainedModelCPU, trainData_cDF, trainLabels_cDF)\n",
    "trainPredictionsGPU, _ = test_model_GPU ( trainedModelGPU, trainData_cDF, trainLabels_cDF)\n",
    "trainAccuracyCPU = accuracy_score(trainLabels_pDF, trainPredictionsCPU)\n",
    "trainAccuracyGPU = accuracy_score(trainLabels_pDF, trainPredictionsGPU)\n",
    "print( 'train data CPU accuracy = {}'.format( trainAccuracyCPU ) )\n",
    "print( 'train data GPU accuracy = {}'.format( trainAccuracyGPU ) )\n",
    "\n",
    "accuracyCPU = accuracy_score(testLabels_pDF, predictionsCPU)\n",
    "accuracyGPU = accuracy_score(testLabels_pDF, predictionsGPU)\n",
    "print( 'test data CPU accuracy = {}'.format( accuracyCPU ) )\n",
    "print( 'test data GPU accuracy = {}'.format( accuracyGPU ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cMat = confusion_matrix(testLabels_pDF, predictionsGPU)\n",
    "print(cMat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the first decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fig = plt.figure(figsize=(100,50))\n",
    "plot_tree(trainedModelGPU, num_trees=0, ax=plt.subplot(1,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expLog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Search          \n",
    "> TODO: ADD early stopping "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kube_ip = !echo $KUBEFLOW_CONTROLLER_SERVICE_HOST\n",
    "if kube_ip[0]:\n",
    "    dask_type = \"k8s\"\n",
    "else:\n",
    "    dask_type = \"local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install dask_kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask import delayed\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask_kubernetes import KubeCluster\n",
    "from dask.distributed import Client\n",
    "from time import sleep # We need to give K8S some time to manually scale workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost;\n",
    "print( xgboost.__version__ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Kubernetes Dask Spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_spec_fname = '/worker_spec.yaml'\n",
    "worker_spec = '''\n",
    "# worker-spec.yml\n",
    "\n",
    "kind: Pod\n",
    "metadata:\n",
    "  labels:\n",
    "    foo: bar\n",
    "spec:\n",
    "  restartPolicy: Never\n",
    "  containers:\n",
    "  - image: supertetelman/k8s-rapids-dask:0.9-cuda10.0-runtime-ubuntu18.04\n",
    "    imagePullPolicy: IfNotPresent\n",
    "    args: [dask-worker,  --nthreads, '1', --no-bokeh, --memory-limit, 6GB, --death-timeout, '60']\n",
    "    name: dask\n",
    "    resources:\n",
    "      limits:\n",
    "        cpu: \"2\"\n",
    "        memory: 6G\n",
    "        nvidia.com/gpu: 1\n",
    "      requests:\n",
    "        cpu: \"2\"\n",
    "        memory: 6G\n",
    "        nvidia.com/gpu: 1\n",
    "'''\n",
    "\n",
    "with open(worker_spec_fname, \"w\") as yaml_file:\n",
    "    yaml_file.write(worker_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_cluster(cluster, n, m=None):\n",
    "    if dask_type != \"k8s\":\n",
    "        print(\"Cannot scale non-K8S cluster\")\n",
    "        return\n",
    "    if m is None:\n",
    "        print(f'Scaling to {n} nodes')\n",
    "        cluster.scale(n)\n",
    "    else:\n",
    "        print(f'Setting Scaling to adapatve, [{n}, {m}]')\n",
    "        cluster.adapt(minimum = n, maximum = m)\n",
    "    sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set number of workers [ changes require kernel restart ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dask_type == \"k8s\":\n",
    "    cluster = KubeCluster.from_yaml(worker_spec_fname)\n",
    "    scale_cluster(cluster, 4)\n",
    "else:\n",
    "    cluster = LocalCUDACluster(ip=\"\", n_workers = 4)\n",
    "\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-scatter Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if client is not None:        \n",
    "    scatteredData_future = client.scatter( [ trainData_cDF, testData_cDF ], broadcast = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatteredData_future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ( '{} \\n {} \\n'.format( scatteredData_future[0].key, scatteredData_future[1].key ) )\n",
    "client.who_has(scatteredData_future)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPO Strategy - Particle Swarm [ Explore + Exploit ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_particles( paramRanges, particlesInTimestep, velocitiesInTimestep, bestParamsIndex, globalBestParams, sBest = .85, sExplore = .1 , deltaTime = 1, randomSeed = None):\n",
    "    \n",
    "    nParticles = particlesInTimestep.shape[ 0 ]\n",
    "    nParameters = particlesInTimestep.shape[ 1 ]    \n",
    "        \n",
    "    globalBestRepeated = numpy.matlib.repmat( np.array( globalBestParams ).reshape( -1, 1 ), nParticles, 1).reshape( nParticles, nParameters )    \n",
    "    \n",
    "    if randomSeed is not None:\n",
    "        np.random.seed(randomSeed)\n",
    "        \n",
    "    # move to best + explore | globalBest + personalBest\n",
    "    velocitiesInTimestep += 0. * velocitiesInTimestep + sBest * ( globalBestRepeated - particlesInTimestep ) \\\n",
    "                            + sExplore * ( np.random.randn( nParticles, nParameters ) )\n",
    "    \n",
    "    particlesInTimestep += velocitiesInTimestep * deltaTime \n",
    "    \n",
    "    # TODO: avoid duplicates\n",
    "    \n",
    "    # enforce param bounds\n",
    "    for iParam in range( nParameters ):\n",
    "        particlesInTimestep[ :, iParam ] = np.clip(particlesInTimestep[ :, iParam ], paramRanges[iParam][1], paramRanges[iParam][2])\n",
    "        if paramRanges[iParam][3] == 'int':\n",
    "            particlesInTimestep[ :, iParam ] = np.round( particlesInTimestep[ :, iParam ] )\n",
    "            \n",
    "    return particlesInTimestep, velocitiesInTimestep\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPO Harness\n",
    "> Particle Evals [ Train and Test Logic ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_hpo ( trainData_cDF, trainLabels_cDF, testData_cDF, testLabels_cDF, particleParams, iParticle, iTimestep ):\n",
    "    \n",
    "    # fixed parameters\n",
    "    paramsGPU = { 'objective': 'binary:hinge',\n",
    "                  'tree_method': 'gpu_hist',\n",
    "                  'n_gpus': 1,\n",
    "                  'random_state': 0 }\n",
    "    \n",
    "    # parameters to search over\n",
    "    paramsGPU['max_depth'] = int(particleParams[0])\n",
    "    paramsGPU['learning_rate'] = particleParams[1]\n",
    "    paramsGPU['lambda'] = particleParams[2]\n",
    "    paramsGPU['num_boost_rounds'] = 1000\n",
    "    \n",
    "    startTime = time.time()\n",
    "    trainDMatrix = xgboost.DMatrix( data = trainData_cDF, label = trainLabels_cDF )\n",
    "    testDMatrix = xgboost.DMatrix( data = testData_cDF, label = testLabels_cDF )\n",
    "    trainedModelGPU = xgboost.train( dtrain = trainDMatrix, evals = [(testDMatrix, 'test')], \n",
    "                                     params = paramsGPU,\n",
    "                                     num_boost_round = paramsGPU['num_boost_rounds'],\n",
    "                                     early_stopping_rounds = 15,\n",
    "                                     verbose_eval = False )\n",
    "    \n",
    "    elapsedTime = time.time() - startTime\n",
    "    \n",
    "    return trainedModelGPU, elapsedTime\n",
    "\n",
    "def test_model_hpo ( trainedModelGPU, trainingTime, testData_cDF, testLabels_cDF ):\n",
    "    \n",
    "    startTime = time.time()\n",
    "    \n",
    "    testDMatrix = xgboost.DMatrix( data = testData_cDF, label = testLabels_cDF )    \n",
    "    predictionsGPU = trainedModelGPU.predict( testDMatrix ).astype(int)\n",
    "    \n",
    "    return predictionsGPU, trainedModelGPU.best_iteration, trainingTime, time.time() - startTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hpo ( daskClient, nTimesteps, nParticles, paramRanges, trainData_cDF, trainLabels_cDF, testData_cDF, testLabels_cDF, randomSeed = 0, plotFlag = True):\n",
    "    \n",
    "    pandasTestLabels = testLabels_cDF.to_pandas()\n",
    "\n",
    "    if daskClient is not None:\n",
    "        scatteredData_future = daskClient.scatter( [ trainData_cDF, trainLabels_cDF, testData_cDF, testLabels_cDF ], broadcast = True )\n",
    "    \n",
    "    trainData_cDF_future = scatteredData_future[0]; trainLabels_cDF_future = scatteredData_future[1]\n",
    "    testData_cDF_future = scatteredData_future[2]; testLabels_cDF_future = scatteredData_future[3]\n",
    "    \n",
    "    particles, velocities, accuracies, bestParticleIndex, \\\n",
    "        globalBestParticleParams, particleBoostingRounds, particleColors = rl.initialize_particle_swarm ( nTimesteps = nTimesteps, \n",
    "                                                                                                          nParticles = nParticles,\n",
    "                                                                                                          paramRanges = paramRanges,\n",
    "                                                                                                          randomSeed = randomSeed, \n",
    "                                                                                                          plotFlag = plotFlag)\n",
    "    globalBestAccuracy = 0\n",
    "    \n",
    "    trainingTimes = np.zeros (( nTimesteps, nParticles ))    \n",
    "    startTime = time.time()\n",
    "    \n",
    "    predictionHistory = np.zeros((nTimesteps, nParticles, testData_cDF.shape[0]))\n",
    "    \n",
    "    for iTimestep in range (0, nTimesteps ):    \n",
    "        if daskClient is not None:\n",
    "            # [ delayed ] train xgboost models on train data\n",
    "            delayedParticleTrain = [ delayed( train_model_hpo )( trainData_cDF_future, trainLabels_cDF_future, \n",
    "                                                                     testData_cDF_future, testLabels_cDF_future, \n",
    "                                                                     particles[iTimestep, iParticle, : ], \n",
    "                                                                         iParticle, iTimestep) for iParticle in range(nParticles) ]\n",
    "\n",
    "            # [ delayed ] determine number of trees/training-rounds returned early stopping -- used to set particle sizes\n",
    "            delayedParticleRounds = [ iParticle[0].best_iteration for iParticle in delayedParticleTrain ]\n",
    "            \n",
    "            # [delayed ] eval trained models on test/validation data\n",
    "            delayedParticlePredictions = [ delayed( test_model_hpo )(iParticle[0], iParticle[1], \n",
    "                                                                     testData_cDF_future, \n",
    "                                                                     testLabels_cDF_future) for iParticle in delayedParticleTrain ]\n",
    "            \n",
    "            # execute delayed             \n",
    "            particlePredictions = dask.compute( delayedParticlePredictions )[0]            \n",
    "            \n",
    "            \n",
    "            for iParticle in range(nParticles):\n",
    "                predictionHistory[iTimestep, iParticle, :] = particlePredictions[iParticle][0]\n",
    "            #import pdb; pdb.set_trace()\n",
    "            \n",
    "            # compute accuracies of predictions\n",
    "            accuracies[iTimestep, :] = [ accuracy_score ( pandasTestLabels, iParticle[0]) for iParticle in particlePredictions ]\n",
    "            particleBoostingRounds[iTimestep, : ] = [ iParticle[1] for iParticle in particlePredictions ]\n",
    "            trainingTimes[iTimestep, :] = [ iParticle[2] for iParticle in particlePredictions ]\n",
    "            del particlePredictions\n",
    "        else:\n",
    "            for iParticle in range(nParticles):\n",
    "                trainedModels, _ = train_model_hpo ( pandasTrainData, pandasTrainLabels, particles[iTimestep, iParticle, : ], iParticle, iTimestep)\n",
    "                predictions, _ = test_model_hpo( trainedModels, pandasTestData, pandasTestLabels)            \n",
    "                accuracies[iTimestep, iParticle] = accuracy_score (pandasTestLabels, predictions)\n",
    "        \n",
    "        bestParticleIndex[iTimestep+1] = np.argmax( accuracies[iTimestep, :] )\n",
    "        currentBestAccuracy = np.max( accuracies[iTimestep, :] )\n",
    "\n",
    "        print('@ hpo timestep : {}, best accuracy is {}'.format(iTimestep, np.max(accuracies[iTimestep, :])) )\n",
    "        if iTimestep +1 < nTimesteps:            \n",
    "            if currentBestAccuracy > globalBestAccuracy:\n",
    "                print('\\t updating best GLOBAL accuracy')\n",
    "                globalBestAccuracy = currentBestAccuracy\n",
    "                globalBestParticleParams[iTimestep+1] = particles[iTimestep, bestParticleIndex[iTimestep+1], :]\n",
    "            else:\n",
    "                globalBestParticleParams[iTimestep+1] = globalBestParticleParams[iTimestep].copy()\n",
    "            \n",
    "            particles[iTimestep+1, :, :], velocities[iTimestep+1, :, : ] = update_particles ( paramRanges, \n",
    "                                                                                              particles[iTimestep, :, :].copy(),\n",
    "                                                                                              velocities[iTimestep, :, :].copy(), \n",
    "                                                                                              bestParticleIndex[iTimestep+1], \n",
    "                                                                                              globalBestParticleParams[iTimestep+1], randomSeed = iTimestep)\n",
    "            \n",
    "\n",
    "    \n",
    "    particleSizes = particleBoostingRounds/np.max(particleBoostingRounds)*10 + 2\n",
    "    \n",
    "    elapsedTime = time.time() - startTime\n",
    "    \n",
    "    bestParamIndex = np.unravel_index(np.argmax(accuracies, axis=None), accuracies.shape)\n",
    "\n",
    "    # best accuracy\n",
    "    print('highest accuracy               :  {} '.format(accuracies[bestParamIndex[0], bestParamIndex[1]]))\n",
    "    print('   @ timestep {}, particle {} '.format( bestParamIndex[0], bestParamIndex[1]))\n",
    "\n",
    "    print('\\nbest model tree depth          :  {} '.format(particles[bestParamIndex[0], bestParamIndex[1], 0]))\n",
    "    print('best model learning rate       :  {} '.format(particles[bestParamIndex[0], bestParamIndex[1], 1]))\n",
    "    print('best model regularization      :  {} '.format(particles[bestParamIndex[0], bestParamIndex[1], 2]))\n",
    "    print('best model num boosting rounds :  {} '.format(int(particleBoostingRounds[bestParamIndex[0], bestParamIndex[1]])))\n",
    "    \n",
    "    print( 'elapsed time : {}'.format(elapsedTime) )\n",
    "    \n",
    "    return accuracies, particles, velocities, particleSizes, particleColors, bestParticleIndex, bestParamIndex, particleBoostingRounds, trainingTimes, predictionHistory, elapsedTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings for HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nTimesteps = 1\n",
    "nParticles = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramRanges = { 0: ['max_depth', 3, 15, 'int'],\n",
    "                1: ['learning_rate', .001, 1, 'float'],\n",
    "                2: ['lambda', 0, 10, 'float'] }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kubernetes let's us dynamically scale the cluster size, which allows us to do these benchmarks\n",
    "if dask_type == \"k8s\":\n",
    "    scale_cluster(cluster, 1)\n",
    "    run_hpo ( client, nTimesteps, nParticles, paramRanges, trainData_cDF,\n",
    "              trainLabels_cDF, testData_cDF, testLabels_cDF, plotFlag=False )\n",
    "    \n",
    "    scale_cluster(cluster, 2)\n",
    "    run_hpo ( client, nTimesteps, nParticles, paramRanges, trainData_cDF,\n",
    "              trainLabels_cDF, testData_cDF, testLabels_cDF, plotFlag=False )\n",
    "    \n",
    "    scale_cluster(cluster, 0, 4)\n",
    "    run_hpo ( client, nTimesteps, nParticles, paramRanges, trainData_cDF,\n",
    "              trainLabels_cDF, testData_cDF, testLabels_cDF, plotFlag=False )\n",
    "    \n",
    "    scale_cluster(cluster, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_cluster(cluster, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies, particles, velocities, particleSizes, particleColors, bestParticleIndex, \\\n",
    "bestParamIndex, particleBoostingRounds, trainingTimes, predictionHistory, elapsedTime = run_hpo ( client, nTimesteps, \n",
    "                                                                                                  nParticles,\n",
    "                                                                                                  paramRanges, \n",
    "                                                                                                  trainData_cDF, trainLabels_cDF, testData_cDF, testLabels_cDF )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl.viz_search( accuracies, particleBoostingRounds )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl.hpo_animate (particles, particleSizes, particleColors, paramRanges, nTimesteps = nTimesteps )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the Prediction Trajectory of the Top Particle [ ending at its best timestep ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl.plot_particle_learning ( nTimesteps, nParticles, testData_pDF, bestParamIndex, predictionHistory )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "##### Jupyter Extensions\n",
    "* [Dask Extension](https://github.com/dask/dask-labextension)\n",
    "* [NVIDIA System Dashboard](https://github.com/jacobtomlinson/jupyterlab-nvdashboard/blob/master/README.md)\n",
    "\n",
    "##### Dask Kubernetes\n",
    "* [Example Notebook](https://github.com/supertetelman/k8s-rapids-dask/blob/master/k8s_examples/Scaling%20Dask%20in%20Kubernetes.ipynb)\n",
    "* [Docker Image](https://github.com/supertetelman/k8s-rapids-dask/blob/master/Dockerfile)\n",
    "* [K8S/Kubeflow/Dask Deployment Guide](https://github.com/NVIDIA/deepops/blob/master/docs/kubernetes-cluster.md)\n",
    "* [Official Docs](https://kubernetes.dask.org/en/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 -- use non-synthetic dataset\n",
    "# 2 -- kubernetes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
